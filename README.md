<div align="right">
**"The limits of my language mean the limits of my world."**
  â€” Ludwig Wittgenstein, _Philosophical Investigations_
</div>

# Prompt Entropy Experiment

Empirical validation of information-theoretic principles in prompt engineering for generative AI systems.

## Research Question
Can we quantify prompt quality using Shannon entropy and mutual information?

## Key Findings
- **Specification-driven prompts** reduce output entropy by 20% (Cohen's d=0.85)
- **Strong MI-entropy correlation** (r=-0.65, p<0.001)
- **Entropy predicts quality** (r=-0.52, p<0.001)

## Dataset
1,800 generations across 30 tasks, 6 domains, 2 models (GPT-4, Claude-3)

## Paper
See `paper/prompt_entropy_paper.tex` for full methodology and results.

**Status**: Exploratory study before deeper categorical operations management framework
