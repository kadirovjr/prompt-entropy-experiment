{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Entropy Experiment: Basic Analysis\n",
    "\n",
    "> \"The limits of my language mean the limits of my world.\"  \n",
    "> â€” Ludwig Wittgenstein\n",
    "\n",
    "This notebook demonstrates the basic workflow for analyzing prompt quality using information-theoretic metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from metrics import (\n",
    "    calculate_all_entropies,\n",
    "    estimate_mutual_information,\n",
    "    calculate_overall_quality\n",
    ")\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Sample Data\n",
    "\n",
    "For demonstration, we'll create synthetic responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example task and prompts\n",
    "task = \"Write a Python function to calculate the Fibonacci sequence up to n terms\"\n",
    "\n",
    "spec_prompt = \"\"\"\n",
    "Write a Python function called 'fibonacci' that:\n",
    "1. Takes an integer n as input (n >= 0)\n",
    "2. Returns a list of the first n Fibonacci numbers\n",
    "3. Handles edge cases: n=0 returns [], n=1 returns [0]\n",
    "4. Uses an iterative approach for efficiency\n",
    "5. Includes docstring with examples\n",
    "6. Has type hints\n",
    "\n",
    "Example output for n=5: [0, 1, 1, 2, 3]\n",
    "\"\"\"\n",
    "\n",
    "vague_prompt = \"Write a fibonacci function\"\n",
    "\n",
    "# Simulate responses (in real analysis, these come from LLM API)\n",
    "spec_responses = [\n",
    "    \"def fibonacci(n: int) -> list[int]:\\n    if n <= 0: return []\\n    if n == 1: return [0]\\n    fib = [0, 1]\\n    for i in range(2, n):\\n        fib.append(fib[-1] + fib[-2])\\n    return fib\",\n",
    "] * 10  # Simulate 10 similar responses\n",
    "\n",
    "vague_responses = [\n",
    "    \"def fib(n): return [fib(i) for i in range(n)]\",\n",
    "    \"# Fibonacci\\ndef fibonacci(n):\\n    a, b = 0, 1\\n    while n > 0:\\n        yield a\\n        a, b = b, a+b\\n        n -= 1\",\n",
    "] * 5  # More diverse responses\n",
    "\n",
    "print(f\"Spec prompt length: {len(spec_prompt)} chars\")\n",
    "print(f\"Vague prompt length: {len(vague_prompt)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate Entropy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate entropies\n",
    "spec_entropy = calculate_all_entropies(spec_responses)\n",
    "vague_entropy = calculate_all_entropies(vague_responses)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "entropy_comparison = pd.DataFrame({\n",
    "    'Specification-driven': spec_entropy,\n",
    "    'Vague': vague_entropy\n",
    "})\n",
    "\n",
    "print(\"\\nEntropy Comparison (bits):\")\n",
    "print(entropy_comparison)\n",
    "\n",
    "# Visualize\n",
    "entropy_comparison.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Entropy Comparison: Spec vs Vague Prompts')\n",
    "plt.ylabel('Entropy (bits)')\n",
    "plt.xlabel('Entropy Metric')\n",
    "plt.legend(title='Prompt Type')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate MI for both prompts\n",
    "spec_mi = estimate_mutual_information(spec_prompt, task)\n",
    "vague_mi = estimate_mutual_information(vague_prompt, task)\n",
    "\n",
    "# Create comparison\n",
    "mi_comparison = pd.DataFrame({\n",
    "    'Specification-driven': spec_mi,\n",
    "    'Vague': vague_mi\n",
    "})\n",
    "\n",
    "print(\"\\nMutual Information Estimates (bits):\")\n",
    "print(mi_comparison)\n",
    "\n",
    "# Visualize\n",
    "mi_comparison.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Mutual Information: Spec vs Vague Prompts')\n",
    "plt.ylabel('MI Estimate (bits)')\n",
    "plt.xlabel('MI Metric')\n",
    "plt.legend(title='Prompt Type')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hypothesis Testing\n",
    "\n",
    "### H1: Specification-driven prompts reduce entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Example with token entropy\n",
    "spec_token_entropy = spec_entropy['token_entropy']\n",
    "vague_token_entropy = vague_entropy['token_entropy']\n",
    "\n",
    "# Calculate effect size (Cohen's d)\n",
    "mean_diff = spec_token_entropy - vague_token_entropy\n",
    "pooled_std = np.sqrt((spec_token_entropy**2 + vague_token_entropy**2) / 2)\n",
    "cohens_d = mean_diff / pooled_std if pooled_std > 0 else 0\n",
    "\n",
    "print(\"\\nH1: Specification-driven prompts reduce entropy\")\n",
    "print(f\"Spec entropy: {spec_token_entropy:.2f} bits\")\n",
    "print(f\"Vague entropy: {vague_token_entropy:.2f} bits\")\n",
    "print(f\"Difference: {mean_diff:.2f} bits\")\n",
    "print(f\"Cohen's d: {cohens_d:.2f}\")\n",
    "print(f\"\\nInterpretation: {'Large' if abs(cohens_d) > 0.8 else 'Medium' if abs(cohens_d) > 0.5 else 'Small'} effect size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Correlation Analysis\n",
    "\n",
    "### H2: Higher MI correlates with lower entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot (with synthetic data for demonstration)\n",
    "np.random.seed(42)\n",
    "n_points = 30\n",
    "\n",
    "# Simulate correlated data\n",
    "mi_values = np.random.uniform(2, 8, n_points)\n",
    "entropy_values = 10 - 0.6 * mi_values + np.random.normal(0, 0.5, n_points)\n",
    "\n",
    "# Calculate correlation\n",
    "r, p_value = stats.pearsonr(mi_values, entropy_values)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(mi_values, entropy_values, alpha=0.6, s=100)\n",
    "plt.xlabel('Mutual Information (bits)')\n",
    "plt.ylabel('Token Entropy (bits)')\n",
    "plt.title(f'MI-Entropy Correlation\\n(r = {r:.2f}, p < .001)')\n",
    "\n",
    "# Add regression line\n",
    "z = np.polyfit(mi_values, entropy_values, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(mi_values, p(mi_values), \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nH2: MI-Entropy Correlation\")\n",
    "print(f\"Pearson r: {r:.2f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(f\"Interpretation: {'Strong' if abs(r) > 0.6 else 'Moderate' if abs(r) > 0.4 else 'Weak'} negative correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Next Steps\n",
    "\n",
    "To complete the full analysis:\n",
    "\n",
    "1. **Data Collection**: Use OpenAI/Anthropic APIs to generate real responses\n",
    "2. **Embeddings**: Generate embeddings for semantic entropy calculation\n",
    "3. **Quality Assessment**: Implement task-specific quality metrics\n",
    "4. **Statistical Analysis**: Run full hypothesis tests across all 30 tasks\n",
    "5. **Visualization**: Create publication-ready figures\n",
    "\n",
    "See the paper (`paper/prompt_entropy_paper.tex`) for full methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Shannon, C. E. (1948). A mathematical theory of communication.\n",
    "- Cover & Thomas (2006). Elements of Information Theory.\n",
    "- Full references in paper LaTeX file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
